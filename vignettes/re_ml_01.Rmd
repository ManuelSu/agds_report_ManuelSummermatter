---
title: "9.4 Report Exercise"
author: "Manuel Summermatter"
date: "2023-05-15"
output: html_document:
---
# Comparison of the linear regression and KNN models
## Introduction
In the first part of this exercise, I analyse linear regression and KNN models and discuss implications of it for different model evaluation metrics. The whole exercise puts a focus on the bias-variance trade off, a very important topic in supervised machine learning.


## Methods and Results
First, I set up the work space
```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(lubridate)
library(caret)
library(recipes)
source(paste0(here::here(), "/R/eval_model.R"))
```

Now, I load and prepare the data (adopted from the chapter)
```{r, message = FALSE, warning = FALSE}
daily_fluxes <- read_csv(paste0(here::here(), "/data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")) |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::na_if(-9999) |> # NOTE: Newer tidyverse version no longer support this statement
                         # instead, use `mutate(across(where(is.numeric), ~na_if(., -9999))) |> `
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
```

Now, I get an overview of the data
```{r, message = FALSE, warning = FALSE}
daily_fluxes |> 
  ggplot(aes(x = GPP_NT_VUT_REF, y = ..count..)) + 
  geom_histogram() +
  theme_classic()
```

What follows are further steps
```{r, message = FALSE, warning = FALSE}
# Data splitting
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())

# Fit linear regression model
mod_lm <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "lm",
  trControl = caret::trainControl(method = "none"),
  metric = "RMSE"
)

# Fit KNN model
mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 8),
  metric = "RMSE"
)
```

Now, I call the model evaluation function, which I loaded into the work space in the beginning, for the linear regression and for the KNN model
```{r, message = FALSE, warning = FALSE}
# Linear regression model
eval_model(mod = mod_lm, df_train = daily_fluxes_train, df_test = daily_fluxes_test)

# KNN
eval_model(mod = mod_knn, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```


## Discussion
The observation that the difference between the evaluation on the training and the test set is larger for the KNN model than for the linear regression model means that the variance of the KNN model is larger than the variance of the linear model. In the light of the bias-variance trade-off, this in turn means that the bias of the KNN model is smaller than of the linear model. This makes sense as the KNN model is more precise than the linear model, which leads to a smaller bias.  

The KNN model suits better in explaining the underlying relationship in the data. This can be seen when observing the R squared values. The KNN model has, for both the training and the test set, larger values of R squared than the linear regression model. This leads to the fact that the KNN model has a better performance on the test set. The higher variance of the KNN model, so the bigger difference from training to test set, cannot change this circumstance, but it makes things less clear on the test set compared to the training set.  

On a spectrum of the bias-variance trade-off, the linear regression model has a high bias and a low variance, and the KNN model has a low bias and a high variance.

I now visualize temporal variations of observed and modelled GPP for both models
```{r, message = FALSE, warning = FALSE}
daily_fluxes_train <- daily_fluxes_train |> 
    drop_na()
  daily_fluxes_train$fitted_lm <- predict(mod_lm, newdata = daily_fluxes_train)
  
daily_fluxes_test <- daily_fluxes_test |> 
    drop_na()
  daily_fluxes_test$fitted_lm <- predict(mod_lm, newdata = daily_fluxes_test)
  
daily_fluxes_train <- daily_fluxes_train |> 
    drop_na()
  daily_fluxes_train$fitted_knn <- predict(mod_knn, newdata = daily_fluxes_train)
  
daily_fluxes_test <- daily_fluxes_test |> 
    drop_na()
  daily_fluxes_test$fitted_knn <- predict(mod_knn, newdata = daily_fluxes_test)

ggplot(data = daily_fluxes_train, aes(x = TIMESTAMP, y = GPP_NT_VUT_REF)) +
  geom_line(aes(color = "Observed GPP")) +
  geom_line(data = daily_fluxes_train, aes(y = fitted_lm, color = "lm on training set")) +
  geom_line(data = daily_fluxes_train, aes(y = fitted_knn, color = "knn on training set")) +
  geom_line(data = daily_fluxes_test, aes(y = fitted_lm, color = "lm on test set")) +
  geom_line(data = daily_fluxes_test, aes(y = fitted_knn, color = "knn on test set")) +
  labs(title = "Temporal variations of observed and modelled GPP", x = "Time", y = "GPP") +
  scale_color_manual(values = c("Observed GPP" = "black", "lm on training set" = "red", "knn on training set" = "green", "lm on test set" = "blue","knn on test set" = "orange")) +
  theme_classic()
```

The visualization shows that all the different time series move very similarly. So, the different predictions of the linear regression as well as the KNN model don't differ much from the observed values of GPP.



# The role of k
## Introduction
In the second part of this exercise, I look at the KNN model in detail and I will examine, which role the hyperparamater k has for the performance of the model.


## Methods and Results
I start this part by formulating two hypotheses. They are solely based on my a priori knowledge.  
- Hypothesis 1: R squared gets smaller as k approaches 1 and higher as k approaches N  
- Hypothesis 2: MAE gets higher as k approaches 1 and lower as k approaches N  
These hypotheses can be explained using the bias-variance trade-off. A small k indicates that the model fits well to the training data, thus resulting in a low bias. This leads to a high variance, meaning the model struggles to explain other data than the training set. This latter fact equals a low R squared and high MAE.

Now, I will put these hypotheses to the test.  
I start off by removing all variables stored in the environment. Then, I read and tidy the data once again
```{r, message = FALSE, warning = FALSE}
rm(list=ls())
source(paste0(here::here(), "/R/eval_model.R"))
daily_fluxes <- read_csv(paste0(here::here(), "/data/FLX_CH-Dav_FLUXNET2015_FULLSET_DD_1997-2014_1-3.csv")) |>  
  
  # select only the variables we are interested in
  dplyr::select(TIMESTAMP,
                GPP_NT_VUT_REF,    # the target
                ends_with("_QC"),  # quality control info
                ends_with("_F"),   # includes all meteorological covariates
                -contains("JSB")   # weird useless variable
                ) |>

  # convert to a nice date object
  dplyr::mutate(TIMESTAMP = ymd(TIMESTAMP)) |>

  # set all -9999 to NA
  dplyr::na_if(-9999) |> # NOTE: Newer tidyverse version no longer support this statement
                         # instead, use `mutate(across(where(is.numeric), ~na_if(., -9999))) |> `
  
  # retain only data based on >=80% good-quality measurements
  # overwrite bad data with NA (not dropping rows)
  dplyr::mutate(GPP_NT_VUT_REF = ifelse(NEE_VUT_REF_QC < 0.8, NA, GPP_NT_VUT_REF),
                TA_F           = ifelse(TA_F_QC        < 0.8, NA, TA_F),
                SW_IN_F        = ifelse(SW_IN_F_QC     < 0.8, NA, SW_IN_F),
                LW_IN_F        = ifelse(LW_IN_F_QC     < 0.8, NA, LW_IN_F),
                VPD_F          = ifelse(VPD_F_QC       < 0.8, NA, VPD_F),
                PA_F           = ifelse(PA_F_QC        < 0.8, NA, PA_F),
                P_F            = ifelse(P_F_QC         < 0.8, NA, P_F),
                WS_F           = ifelse(WS_F_QC        < 0.8, NA, WS_F)) |> 

  # drop QC variables (no longer needed)
  dplyr::select(-ends_with("_QC"))
```

Now, I continue with further steps, as above
```{r, message = FALSE, warning = FALSE}
set.seed(1982)  # for reproducibility
split <- rsample::initial_split(daily_fluxes, prop = 0.7, strata = "VPD_F")
daily_fluxes_train <- rsample::training(split)
daily_fluxes_test <- rsample::testing(split)

# Model and pre-processing formulation, use all variables but LW_IN_F
pp <- recipes::recipe(GPP_NT_VUT_REF ~ SW_IN_F + VPD_F + TA_F, 
                      data = daily_fluxes_train |> drop_na()) |> 
  recipes::step_BoxCox(all_predictors()) |> 
  recipes::step_center(all_numeric(), -all_outcomes()) |>
  recipes::step_scale(all_numeric(), -all_outcomes())
```

I now fit and evaluate the KNN model, first with a small k and then with a large k
```{r, message = FALSE, warning = FALSE}
# KNN model fitting and evaluation with small k:
mod_knn_small_k <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 2),
  metric = "RMSE"
)
eval_model(mod = mod_knn_small_k, df_train = daily_fluxes_train, df_test = daily_fluxes_test)

# KNN model fitting and evaluation with large k:
mod_knn_large_k <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = 16),
  metric = "RMSE"
)
eval_model(mod = mod_knn_large_k, df_train = daily_fluxes_train, df_test = daily_fluxes_test)
```


## Discussion
First, the two plots speak in favor of Hypothesis 1: For a small k (approaching 1), R squared drops significantly from the training to the test set (here from 0.85 to 0.56). This is a substantial difference. For a large k (approaching N), R squared decreases only a bit from the training to the test set (here from 0.69 to 0.65). So the model generalizes much better (i.e. has a larger R squared on the test set) for large k than small k. I recall that in the light of the bias-variance trade off, large k means high bias and therefore small variance, and small variance equals good generalisability.  
Second, the two plots also support Hypothesis 2: In the model with small k, MAE increases by a significant amount from the training to the test set (from 0.759 to 1.35). For a large k, MAE only increases a little (from 1.08 to 1.14). So on the test set, the MAE is much higher for the model with small k. This again supports the fact that small k implies a low bias, but a high variance and therefore not a good generalisability.

I continue with building a function that takes k as an input and returns the MAE of the model on the test set
```{r, message = FALSE, warning = FALSE}
k_MAE <- function(k) {
  mod_knn <- caret::train(
  pp, 
  data = daily_fluxes_train |> drop_na(), 
  method = "knn",
  trControl = caret::trainControl(method = "none"),
  tuneGrid = data.frame(k = k),
  metric = "RMSE"
)
  
  daily_fluxes_test <- daily_fluxes_test |> 
    drop_na()
  daily_fluxes_test$fitted <- predict(mod_knn, newdata = daily_fluxes_test)
  
  metrics_test <- daily_fluxes_test |> 
    yardstick::metrics(GPP_NT_VUT_REF, fitted)
  
  as.double(format(metrics_test |> 
    filter(.metric == "mae") |> 
    pull(.estimate), digits = 4))
}

k_MAE(19)
```

I finally visualize the previous findings. I do this by firstly computing MAE values for different levels of k (which is implemented by a loop iterating over the self made function k_MAE, and then visualizing these outcomes (note: The loop takes a long time to finish)
```{r, message = FALSE, warning = FALSE}
result_MAE <- c()
k <- 1:499

for (i in 1:499) {
  result_MAE[i] <- k_MAE(i)
}

ggplot() +
  geom_point(aes(x = k, y = result_MAE)) +
  labs(title = "MAE as a function of k", x = "k", y = "MAE") +
  theme_classic()
```

In this plot, the x axis, so the number of k, can be viewed as the model complexity. The y axis, the values of the MAE, can be viewed as the model generalisability. One can obtain from the plot, that there exists a level of k, for which this trade off is balanced optimally. The area of small values of k is a region where overfitting occurs, whereas there occurs underfitting in the region of high values of k.

Lastly, I want to find the optimal k in terms of model generalisability.
```{r, message = FALSE, warning = FALSE}
df <- data.frame("k" = k, "MAE" = result_MAE)
knitr::kable(df |>
  filter(MAE == min(MAE))
,caption = "Minimal MAE and its respective k")
```

It can be obtained that there are two values of k for which the MAE is minimal: k = 19 and k = 33. These values of k serve as the optimal values in terms of model generalisability